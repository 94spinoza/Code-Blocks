{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "캡챠 우회해서 웹 스크래핑/ 크롤링하기.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNM9LQuXLvg7MsO9PgM89GS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/94spinoza/Code-Blocks/blob/main/%EC%BA%A1%EC%B1%A0_%EC%9A%B0%ED%9A%8C%ED%95%B4%EC%84%9C_%EC%9B%B9_%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91_%ED%81%AC%EB%A1%A4%EB%A7%81%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCsrIpCY7uJC"
      },
      "source": [
        "오늘 크롤링을 하다가 캡챠로 인한 오류를 처음으로 경험했다.\n",
        "\n",
        "**0\\. 문제 상황**\n",
        "\n",
        "[onlinelibrary.wiley.com/journal/10970266](https://onlinelibrary.wiley.com/journal/10970266)\n",
        "\n",
        "\n",
        "위 논문 저널에서 경영학 논문의 키워드를 스크래핑하여 추세를 살펴보려고 했다.\n",
        "\n",
        "그래서 아래와 같은 코드를 작성해서 돌렸다.\n",
        "\n",
        "```\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "b_url = \"https://onlinelibrary.wiley.com/action/doSearch?SeriesKey=10970266&content=articlesChapters&countTerms=true&pageSize=2000&target=default&\"\n",
        "\n",
        "ay = \"AfterYear=\"\n",
        "by = \"&BeforeYear=\"\n",
        "a_year = 2021\n",
        "b_year = 2021\n",
        "period = ay + str(a_year) + by + str(b_year)\n",
        "\n",
        "url = b_url + period\n",
        "print(url)\n",
        "\n",
        "# Soup 만들기\n",
        "r = requests.get(url)\n",
        "soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "# 태그 접근하기\n",
        "link = soup.get(\"li\")\n",
        "print(link)\n",
        "```\n",
        "\n",
        "그랬더니 None값이 나오는 것이다.\n",
        "\n",
        "페이지 소스에서 태그를 확인했기에 태그 명칭의 문제는 아니다.\n",
        "\n",
        "그래서 크롤링을 위해 만들어 둔 soup을 확인했다.\n",
        "\n",
        "```\n",
        "<!DOCTYPE html>\n",
        "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]--><!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]--><!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]--><!--[if gt IE 8]><!--><html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
        "<head>\n",
        "<title>Attention Required! | Cloudflare</title>\n",
        "<meta id=\"captcha-bypass\" name=\"captcha-bypass\"/>\n",
        "<meta charset=\"utf-8\"/>\n",
        "```\n",
        "\n",
        "확인해보니 페이지 소스와는 다른 내용들이 담겨 있었다.\n",
        "\n",
        "위에 첨부한 부분은 상단의 일부분이다.\n",
        "\n",
        "읽어보면 Cloudflare 관련 주의가 뜬 것을 알 수 있다.\n",
        "\n",
        "좀 더 구체적인 문제는 captcha-bypass인 것 같다.\n",
        "\n",
        "**1\\. 문제 원인**\n",
        "\n",
        "이러한 문제는 사이트에서 스크래핑을 막기 때문이다.\n",
        "\n",
        "스크래핑 자체를 막는 것은 아니다.\n",
        "\n",
        "비정상적인 속도의 서버 접근을 막는 것이다.\n",
        "\n",
        "이를 막지 않으면 서버는 쉽게 다운될 수 있다.\n",
        "\n",
        "여러 컴퓨터에서 접근을 어마무지하게 요청하면 서버는 무너진다.\n",
        "\n",
        "**2\\. 문제 해결**\n",
        "\n",
        "cloudscraper 패키지를 설치하면 문제가 해결된다.\n",
        "\n",
        "해당 패키지는 cloudflare의 캡챠를 우회하게 해준다.\n",
        "\n",
        "Colab 혹은 Jupyter Notebook 코드블록에다가\n",
        "\n",
        "_pip install cloudscraper_\n",
        "\n",
        "위 한 줄을 치고 실행시키면 설치가 된다.\n",
        "\n",
        "공식 사이트는 아래 링크를 타고 가면 된다.\n",
        "\n",
        "[pypi.org/project/cloudscraper/](https://pypi.org/project/cloudscraper/)\n",
        "\n",
        "[\n",
        "\n",
        "cloudscraper\n",
        "\n",
        "A Python module to bypass Cloudflare's anti-bot page.\n",
        "\n",
        "pypi.org\n",
        "\n",
        "\n",
        "\n",
        "](https://pypi.org/project/cloudscraper/)\n",
        "\n",
        "이제 캡챠 에러 없이 스크래핑이 가능할 것이다.\n",
        "\n",
        "**3\\. 최종 코드**\n",
        "\n",
        "```\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install cfscrape\n",
        "import cfscrape\n",
        "\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "b_url = \"https://onlinelibrary.wiley.com/action/doSearch?SeriesKey=10970266&content=articlesChapters&countTerms=true&pageSize=2000&target=default&\"\n",
        "\n",
        "# 3년 간격\n",
        "ay = \"AfterYear=\"\n",
        "by = \"&BeforeYear=\"\n",
        "b_year = 1982\n",
        "a_year = b_year-2\n",
        "\n",
        "period = ay+str(a_year)+by+str(b_year)\n",
        "range = str(a_year)+\"-\"+str(b_year)\n",
        "url = b_url+period\n",
        "\n",
        "keywords_dict = {}\n",
        "keywords_dict[range] = {}\n",
        "\n",
        "# Soup 만들기\n",
        "scraper = cfscrape.create_scraper()\n",
        "r = scraper.get(url)\n",
        "soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "# 태그 접근하기\n",
        "link_class = soup.find_all(attrs={'class':'hlFld-Title'})\n",
        "\n",
        "# Link 추출하기\n",
        "link = []\n",
        "\n",
        "for i in link_class:\n",
        "  res = i.find(\"a\")[\"href\"]\n",
        "  link.append(res)\n",
        "\n",
        "# 논문 Keyword 따기\n",
        "base_url = 'https://onlinelibrary.wiley.com'\n",
        "\n",
        "keywords = []\n",
        "\n",
        "for i in link:\n",
        "  url = base_url+str(i)\n",
        "  r = scraper.get(url)\n",
        "  soup = BeautifulSoup(r.text, 'lxml')\n",
        "  link_class = soup.find_all(attrs={'name':'citation_keywords'})\n",
        "  for name in link_class:\n",
        "    key_word = name[\"content\"]\n",
        "    keywords.append(key_word)\n",
        "    \n",
        "for word in keywords:\n",
        "  year_dict = keywords_dict[range]\n",
        "  try:\n",
        "    year_dict[word] += 1\n",
        "  except KeyError:\n",
        "    year_dict[word] = 1\n",
        "  keywords_dict[range].update(year_dict)\n",
        "\n",
        "# Text 파일에 기록하고 저장\n",
        "title = \"논문 키워드 추출_\"+str(a_year)+\"_\"+str(b_year)+\".txt\"\n",
        "f = open(title, \"w\", newline='')\n",
        "keywords_string = json.dumps(keywords_dict)\n",
        "\n",
        "f.write(keywords_string)\n",
        "f.close\n",
        "\n",
        "files.download(title)\n",
        "```\n",
        "\n",
        "모두 행복하길\n",
        "\n",
        "\\-끝-"
      ]
    }
  ]
}